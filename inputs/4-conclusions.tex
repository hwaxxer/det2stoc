\chapter{Conclusions}
\label{conclusions}

\section{Summary}

The proposed method in this thesis is built on the basis of the \vae{} and its conditionally modified variant \cvae{}. The \cvae{} was used to learn latent representations that can be used to ameliorate Sim-To-Real transfer by more accurately modelling reality in a data-efficient way. We incorporated the various techniques to avoid latent variable collapse. A baseline \cvae{} was trained on real data and required at least three times as many samples to be on par with \dettostoc{}.

The \dettostoc{} algorithm was tested on two MuJoCo scenarios. The two scenarios were similar, but the first scenario only simulated passive dynamics while the second one included a robot interacting with the system with control actions. In both scenarios, the \dettostoc{} exceedingly outperformed the baseline and correctly identified the distribution of the selected parameters to almost identical means and variances. On average, training a \cvae{} from scratch required four times as much data to achieve the same log likelihood of the real data.

We showed that the \dettostoc{} algorithm can be used as a data-efficient way to represent probability dynamics of robots and their environments.

The code for \dettostoc{}, as well as both MuJoCo environments can be found at \url{https://github.com/hwaxxer/det2stoc}. 

\section{Discussion}

From the results, it is clear that \dettostoc{} can be used to improve performance of a stochastic simulator. 

One problem when learning the dynamics of a scenario with control actions, such as an RL environment, is that some states will not be explored in reasonable time with purely random actions. In the YuMi Box Pushing scenario, we trained an RL policy using dynamics randomization and collected data during rollouts of the trained policy. The policy can then be transferred to the real-world whereupon we collect real data and train using \dettostoc{}. This process can be repeated many times.
% can locate the mean but slightly underestimates the variance of parameters. This is a general problem of variational inference \parencite{bishop:2006:PRML} and not specific to the proposed methods.

\subsection{Constrained optimization}

The \dettostoc{} algorithm that has been outlined in this thesis does not enforce validness of the output. The loss used to optimize the predictions is the euclidean norm together with a divergence term. This means that no measures have been taken to ensure that the properties of the predicted output are valid. For example, if the output is a rotation matrix, it is not guaranteed to be orthogonal. However, this is not a limitation of the proposed \dettostoc{} algorithm, but rather a general problem when training neural networks. Moreover, this does not have any significant impact for the uses described in this thesis, but should be considered if the output is used in other systems that require correctness. %Different measures can be taken to avoid this problem. For example, if the rotation matrix $M$ needs to be orthogonal: let $M=U \Sigma V$ be the singular value decomposition of $M$, then $R=UV$.

\subsection{Future work}

For the experiments, we assume that the \textit{real} parameters fall within the interval of a prior estimation. Future research should look into whether \dettostoc{} can indicate that the prior is outside the interval, and how well it handles more uninformed priors for the parameters.

Some work was done trying out a mixture of distributions as prior \parencite{DBLP:journals/corr/DilokthanakulMG16} but we found this approach ineffective in our experiments.

We used a \cvae{} as a generative model. However, \dettostoc{} could be used with other generative models, such as Generative Adversarial Networks (GANs) \parencite{goodfellow2014} with some modifications.

We would like to extend our framework to real data and real robots in future work. Furthermore, we would incorporate higher-dimensional sensor modalities such as vision for both state observations and parameters of simulation randomization.
Another interesting direction would be to employ a higher latent space dimension than the dimension of parameters. This would allow for capturing latent information that is not directly tied to the parameters we chose to learn and could yield even better performance.